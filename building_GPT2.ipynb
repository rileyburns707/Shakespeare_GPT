{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7g8gXE+KrlZQ9NDwRxIb6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rileyburns707/Shakespeare_GPT/blob/main/building_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWnsp-EIRp6a",
        "outputId": "64d08330-dcb7-4d61-cf0d-9264abcd4ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-06 16:34:00--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-06-06 16:34:00 (17.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "# this loads in the tiny shakespeare dataset. We will train the model off of this\n",
        "# input.txt is the name of the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it and read in the data as a string. Opens the file\n",
        "# reading it in is the \"input.txt\". That is the data set\n",
        "# 'r' means you open the input.txt file in read mode\n",
        "# Encoding means it specifies that the file should be read using the UTF-8 encoding\n",
        "# It reads the entire content of the file into a variable named text\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# The open function returns a file object, which is assigned to the variable f. The with statement\n",
        "# ensures that the file is closed automatically when the block inside it is exited\n",
        "# After this code executes, the text variable will contain all the text from input.txt,\n",
        "# and the file will be closed automatically, ensuring there are no resource leaks."
      ],
      "metadata": {
        "id": "dqnGKombR70A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))\n",
        "# prints the amount of characters in the dataset. roughly 1 million characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_0wgtisT8gb",
        "outputId": "4f52b97e-775d-4fea-fa13-6bfc4da5d104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at the first 1000 charactera\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhRxBKUhUF4Y",
        "outputId": "95bb5549-8b5d-490f-c8f5-60a10b0e91d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "\n",
        "chars = sorted(list(set(text))) # Gets all the characters that occur in the data set sorted\n",
        "# text is a sequence of characters in python. The set constructor gets a set of all the characters\n",
        "# that occur in this text. The list function orders it arbitrarily. Sorted sorts them\n",
        "\n",
        "vocab_size = len(chars) # the number of them. The possible elements of the sequences\n",
        "\n",
        "print(''.join(chars)) # prints all the characters\n",
        "print(vocab_size) # says the number of characters. In this case it is 65\n",
        "\n",
        "# Each charcter printed below are all the possibe characters that occured in the dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eGBOvJ0UXMW",
        "outputId": "f8327885-8195-40dd-a51e-cc3fec98322a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "\n",
        "# stoi() means 'string to integer' - is a function that converts a string to an integer value and returns that value\n",
        "stoi = { ch:i for i, ch in enumerate(chars) } # create look up table from character to integer\n",
        "# creates a dictionary (stoi) that maps each character in the chars string to a unique integer index\n",
        "# The enumerate function is used to get both the index and the character\n",
        "# The dictionary comprehension { ch:i for i, ch in enumerate(chars) } iterates over each character in chars,\n",
        "# assigning the character (ch) as the key and its index (i) as the value.\n",
        "\n",
        "itos = { i:ch for i, ch in enumerate(chars) } # create look up table from integer to the character\n",
        "# creates a dictionary (itos) that maps each unique integer index back to the corresponding character.\n",
        "# the dictionary comprehension { i:ch for i, ch in enumerate(chars) } assigns the index (i) as the key\n",
        "# and the character (ch) as the value.\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "# defines an anonymous function (a lambda function) called encode that takes a string (s) as input and returns a list of integers\n",
        "# [stoi[c] for c in s] iterates over each character (c) in the input string (s),\n",
        "# looks up the integer index for that character in the stoi dictionary, and\n",
        "# constructs a list of these integer indices.\n",
        "\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "# [itos[i] for i in l] iterates over each integer index (i) in the input list (l), looks up the corresponding\n",
        "# character in the itos dictionary, and constructs a list of these characters.\n",
        "# ''.join(...) concatenates these characters into a single string.\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n",
        "# This ensures that the original string is correctly transformed to a list of integers and back.\n",
        "# We now effectivly have a tokenizer!!"
      ],
      "metadata": {
        "id": "wMuaAHiLU4xc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a5fd50e-e965-482d-cab3-6076775dfbad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "# gets all the text in tinyshakespeare, encodes it, then wrap it into a torch.tensor, this gets the data tensor\n",
        "# A tensor is a multi-dimensional array (similar to NumPy arrays) and is the fundamental data structure in PyTorch\n",
        "# encode(text) produces a list of integers\n",
        "# torch.tensor(encode(text), dtype=torch.long) converts this list into a 1-dimensional tensor of integers\n",
        "# The argument dtype=torch.long specifies that the data type of the tensor elements should be long integers (64-bit integers).\n",
        "\n",
        "print(data.shape, data.dtype)\n",
        "# data.shape returns the dimensions of the tensor\n",
        "# Since encode(text) produces a 1-dimensional list, data.shape will be a tuple with one element representing the length of the encoded text.\n",
        "# data.dtype returns the data type of the tensor elements, which will be torch.int64 (another name for torch.long).\n",
        "\n",
        "print(data[:1000]) # the 1000 characters we looked at earlier will to the GPT look like this\n",
        "\n",
        "# this is an identical translation of the first 1000 characters we printed above"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi5YilFmPAh5",
        "outputId": "6f4cb6bd-8882-4591-cf45-6d60400fe9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now split up the data into train and validation sets. I know it as train test split\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val (test)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:] # test data we will use to see how accurate the model is. Don't train of this data"
      ],
      "metadata": {
        "id": "WyrlIbj4XBwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # trains the model on chunks of the data since training on all the data is compuationally expensive\n",
        "train_data[:block_size+1] # first 9 characters in training set. 8 chunks\n",
        "# has multiple examples packed into it because they all connect.\n",
        "# for example in the conext of '18', '47' likely comes next\n",
        "# in the conext of '18' and '47', '56' likely comes next. And so on\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkuAhoEwb34V",
        "outputId": "10bf8f70-578d-4164-afd8-8caad94d7cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spells out in code what I just explained\n",
        "# prints the 8 examples hidden in the chunk of 9 characters\n",
        "x = train_data[:block_size] # inputs to the transformer. 1st block_size characters\n",
        "y = train_data[1:block_size+1] # targets for each position. Next block_size characters. Offset by 1\n",
        "for t in range(block_size): # iterating over the block_size of 8\n",
        "  context = x[:t+1] # all the characters in x up to 't' including 't'\n",
        "  target = y[t] # always the t'th character\n",
        "  print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGIHYDvJcnme",
        "outputId": "996339c8-754e-42b1-d619-505f1b8e0f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Dimension: The batch dimension allows processing multiple sequences in parallel. Instead of processing one sequence at a time, you process a set of sequences simultaneously. This is more efficient and makes better use of modern hardware, which is optimized for parallel computations. In this example, batch_size = 4 means you are working with 4 sequences at once, each of length block_size = 8.\n"
      ],
      "metadata": {
        "id": "gGFZ9uljrWT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generalize what is above and introduce a batch dimension\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "# sets the random seed to 1337 for PyTorch's random number generator\n",
        "# ensures that the random numbers generated in the code are the same each time you run it\n",
        "\n",
        "\n",
        "batch_size = 4 # determines how many sequences of data will be processed in parallel.\n",
        "# A batch size of 4 means we'll be working with 4 sequences simultaneously.\n",
        "\n",
        "block_size = 8 # defines the length of each sequence.\n",
        "# Each sequence will have a context length of up to 8 characters for prediction purposes.\n",
        "\n",
        "# Defines a function to generate batches of data\n",
        "def get_batch(split):\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,)) # gets 4 numbers that are randomly generated between 0 and len(data) - block_size\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix]) # first block size characters starting at i\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # offset by 1 of x\n",
        "  return x, y\n",
        "\n",
        "# split: Indicates whether to use training data (train_data) or validation data (val_data).\n",
        "# data: Selects the appropriate dataset based on the split parameter.\n",
        "# ix: Generates a tensor of random starting indices.\n",
        "      # generates batch_size random integers between 0 and len(data) - block_size.\n",
        "      # These integers represent the starting points of the sequences.\n",
        "# x: Creates a tensor of input sequences by stacking slices of the data from the indices in ix to ix + block_size.\n",
        "# y: Creates a tensor of target sequences by stacking slices of the data from ix + 1 to ix + block_size + 1.\n",
        "      # These are the next characters that the model should predict.\n",
        "\n",
        "xb, yb = get_batch('train') # xb (inputs) and yb (targets) are tensors with shapes (batch_size, block_size)\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "# get an input and output matrix. Using Linear Algebra concpets here to make data easy to read\n",
        "\n",
        "print('----')\n",
        "\n",
        "# spell out in code what I just explained\n",
        "for b in range(batch_size): # batch dimension\n",
        "  for t in range(block_size): # time dimension\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"When input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7iU_eX8eAVl",
        "outputId": "e9457c64-94f8-4975-cdff-dc1800f1d03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "When input is [24] the target: 43\n",
            "When input is [24, 43] the target: 58\n",
            "When input is [24, 43, 58] the target: 5\n",
            "When input is [24, 43, 58, 5] the target: 57\n",
            "When input is [24, 43, 58, 5, 57] the target: 1\n",
            "When input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "When input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "When input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "When input is [44] the target: 53\n",
            "When input is [44, 53] the target: 56\n",
            "When input is [44, 53, 56] the target: 1\n",
            "When input is [44, 53, 56, 1] the target: 58\n",
            "When input is [44, 53, 56, 1, 58] the target: 46\n",
            "When input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "When input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "When input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "When input is [52] the target: 58\n",
            "When input is [52, 58] the target: 1\n",
            "When input is [52, 58, 1] the target: 58\n",
            "When input is [52, 58, 1, 58] the target: 46\n",
            "When input is [52, 58, 1, 58, 46] the target: 39\n",
            "When input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "When input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "When input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "When input is [25] the target: 17\n",
            "When input is [25, 17] the target: 27\n",
            "When input is [25, 17, 27] the target: 10\n",
            "When input is [25, 17, 27, 10] the target: 0\n",
            "When input is [25, 17, 27, 10, 0] the target: 21\n",
            "When input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "When input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "When input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqFATet1k4Ak",
        "outputId": "825037fe-17f3-4363-f703-6abd92349eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram language model\n",
        "\n",
        "# import PyTorch model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337) # for reproducability\n",
        "\n",
        "class BigramLanguageModel(nn.Module): # constructing a Bigram Language Model\n",
        "\n",
        "\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a loopup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "\n",
        "  # evauluates the quality of the model\n",
        "  def forward(self, idx, targets = None): # input is rennamed to idx. Targets is optional so it equals none\n",
        "\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "    logits = self.token_embedding_table(idx) # (B,T,C) (Batch by Time by Chanel tensor)...batch = 4, time = 8, chanel = vocab_size = 65\n",
        "            # going to pluck all the rows (24, 43,...), arrange them in a (B,T,C)\n",
        "            # and interpret this as the logits, which are basically the scores for the next charater in the sequence\n",
        "            # we are predicting what comes next based on the individual identity of a sigle token\n",
        "\n",
        "    if targets is None: # if we have targets we provide them and get a loss. If we have no targets it will just get the logits\n",
        "      loss = None\n",
        "    else:\n",
        "        B,T,C = logits.shape # unpacks those numbers\n",
        "        logits = logits.view(B*T, C) # 2D array. Streches orignal array to better conform to what PyTorch expects in its dimensions\n",
        "        targets = targets.view(B*T) # 1D array\n",
        "\n",
        "        loss = F.cross_entropy(logits, targets) # measures the quality of the logits w/ respect to the targets\n",
        "              # the correct dimension of logits should have a very high number, and all the other dimensions should be a low number\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "    # Generates from the model. Generate is [(B, T+1), (B, T+2), ...].\n",
        "    # Continues the generation of all the batch dimensions in the time dimension\n",
        "    # will do that for max_new_tokens\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "      # idx is (B,T) array of indices in the current context\n",
        "      for _ in range(max_new_tokens):\n",
        "          # get the predictions\n",
        "          logits, loss = self(idx) # loss is ignored here\n",
        "\n",
        "          # focus only the last time step by getting last element in time dimension\n",
        "          logits = logits[:, -1, :] # becomes (B, C)\n",
        "\n",
        "          # apply softmax to get probabilties\n",
        "          probs = F.softmax(logits, dim = -1) # (B, C)\n",
        "\n",
        "          # sample from the distribution\n",
        "          idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
        "              # samples from probabilities and ask PyTorch to give us 1 sample\n",
        "              # this will give us a single prediction for what comes next\n",
        "\n",
        "          # append sampled index to the running sequence\n",
        "          idx = torch.cat((idx, idx_next), dim = 1) # (B, T+1)\n",
        "              # whatever is predicted is concatenated on top of the previous idx along the\n",
        "              # first dimension along the time dimension which creates B by T+1\n",
        "      return idx\n",
        "    # right now the generate function is a overkill since we only need the character right before the\n",
        "    # prediction but we fed in all the previous characters. Later we will use the history so it will\n",
        "    # make sense to use this set up, for now it is just creating a good draft\n",
        "\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb) # calling the Bigram Language Model and passing the inputs and targets\n",
        "print(logits.shape)\n",
        "\n",
        "print(loss) # loss is 4.87. Should be -ln(1/65) = 4.17.\n",
        "          # tells us that the inital conditions are not super disfuse so we have some entropy\n",
        "          # This evaluates the quality of the\n",
        "\n",
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "      # batch=1, time=1 therefore creating 1x1 tensor and it is holding a 0\n",
        "      # dtype is the data type which is integer\n",
        "      # 0 is how we start the generation. Which in this case a 0 is the new line character which is a reasonable thing to feed in at the start\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
        "      # after idx we will ask for 100 tokens\n",
        "      # tolist converts to a simple python list that can feed into the decode function above\n",
        "      # the output is garabage because it is a totally random model. So we want to train this model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL7lm2cE46SW",
        "outputId": "2047bbe9-8d2f-4f70-ba89-e862a51dfa08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model so it is less random\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) # lr = learning rate"
      ],
      "metadata": {
        "id": "RvzGmmqc60kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# will take the gradients and update the parameters using the gradients\n",
        "\n",
        "# typical training loop\n",
        "batch_size = 32\n",
        "for steps in range(100): # for some number of steps\n",
        "\n",
        "  xb, yb, = get_batch('train') # we are sampling a new batch of data\n",
        "\n",
        "  logits, loss = m(xb,yb) # evaluating the loss\n",
        "  optimizer.zero_grad(set_to_none=True) # zeroing out all the gradients from the previous step\n",
        "  loss.backward() # getting the gradients for all the parameters\n",
        "  optimizer.step() # using those gradients to update our parameters\n",
        "\n",
        "  print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-VhLdleK3vn",
        "outputId": "b64e1d3c-6b20-47a6-f1de-7a4fe2225abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.704006195068359\n",
            "4.721118927001953\n",
            "4.653193473815918\n",
            "4.706261157989502\n",
            "4.780904293060303\n",
            "4.751267910003662\n",
            "4.8395490646362305\n",
            "4.667973041534424\n",
            "4.743716716766357\n",
            "4.774043083190918\n",
            "4.6908278465271\n",
            "4.789142608642578\n",
            "4.61777925491333\n",
            "4.650947093963623\n",
            "4.886447429656982\n",
            "4.703796863555908\n",
            "4.757591724395752\n",
            "4.655108451843262\n",
            "4.709283828735352\n",
            "4.6745147705078125\n",
            "4.760501384735107\n",
            "4.7892632484436035\n",
            "4.653748512268066\n",
            "4.6619181632995605\n",
            "4.673007488250732\n",
            "4.66577672958374\n",
            "4.7301106452941895\n",
            "4.755304336547852\n",
            "4.712186813354492\n",
            "4.745501518249512\n",
            "4.726755619049072\n",
            "4.735108375549316\n",
            "4.777461051940918\n",
            "4.643350601196289\n",
            "4.6651835441589355\n",
            "4.79764461517334\n",
            "4.717412948608398\n",
            "4.683647155761719\n",
            "4.81886100769043\n",
            "4.613771915435791\n",
            "4.573785781860352\n",
            "4.560741901397705\n",
            "4.81563138961792\n",
            "4.6061553955078125\n",
            "4.619696140289307\n",
            "4.725419521331787\n",
            "4.650487899780273\n",
            "4.5941481590271\n",
            "4.7202863693237305\n",
            "4.699342250823975\n",
            "4.6724138259887695\n",
            "4.727972984313965\n",
            "4.66152286529541\n",
            "4.616766929626465\n",
            "4.599857807159424\n",
            "4.6533403396606445\n",
            "4.716132164001465\n",
            "4.692666053771973\n",
            "4.6675333976745605\n",
            "4.655758857727051\n",
            "4.655789375305176\n",
            "4.754217624664307\n",
            "4.723147869110107\n",
            "4.617090702056885\n",
            "4.704502582550049\n",
            "4.752079486846924\n",
            "4.569591999053955\n",
            "4.547887802124023\n",
            "4.571099281311035\n",
            "4.569430828094482\n",
            "4.598389148712158\n",
            "4.547847747802734\n",
            "4.591439247131348\n",
            "4.6599297523498535\n",
            "4.623749732971191\n",
            "4.742475509643555\n",
            "4.644272804260254\n",
            "4.642965316772461\n",
            "4.579803943634033\n",
            "4.583383560180664\n",
            "4.721978187561035\n",
            "4.669403076171875\n",
            "4.630046367645264\n",
            "4.55496883392334\n",
            "4.681591987609863\n",
            "4.6749467849731445\n",
            "4.65585994720459\n",
            "4.669306755065918\n",
            "4.64952278137207\n",
            "4.674875736236572\n",
            "4.6534528732299805\n",
            "4.785573959350586\n",
            "4.731050968170166\n",
            "4.596864700317383\n",
            "4.586680889129639\n",
            "4.762486934661865\n",
            "4.741381645202637\n",
            "4.602567672729492\n",
            "4.662181854248047\n",
            "4.587916374206543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same thing as above but this time increasing the steps and only print at the end\n",
        "batch_size = 32\n",
        "for steps in range(1000):\n",
        "\n",
        "  xb, yb, = get_batch('train')\n",
        "\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "# Run 1 outputted 3.6\n",
        "# Run 2 outputted 3.06\n",
        "# Run 3 outputted 2.68\n",
        "# so it is optimzing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYnjpcu9L9HS",
        "outputId": "faab20dc-1e8e-4d2b-df98-fad607f429b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.6380467414855957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same thing as above but this time increasing the steps since rerunning 10 times was silly\n",
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "\n",
        "  xb, yb, = get_batch('train')\n",
        "\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "# Output was 2.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHtSo-5JMjTC",
        "outputId": "9dd84a70-5bf2-4303-89b3-be413920d8cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4199717044830322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMfbaCKkM-Jv",
        "outputId": "f8a2e000-8c7d-4647-e441-d5ee0aab5e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "weangond\n",
            "OMave wap\n",
            "\n",
            "I RO:\n",
            "Banleenoalit-blt\n",
            "INRon\n",
            "\n",
            "UM: nd kngonesll;\n",
            "O: pa heore 'ga llis?-sur inidi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=400)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaCQ77ntNqwS",
        "outputId": "cb2a262a-6c75-43aa-e454-86f3bab57e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ANGO:\n",
            "He rthay n thavee\n",
            "Sw s serer Fofow.\n",
            "Houspathe t:\n",
            "Mind fit.\n",
            "DUKINoceamy hun.\n",
            "CKIUShorst onre t ache bar, simed?\n",
            "And me theluse BHENurind-g'sto f w m CK:\n",
            "YCESI fatass mbre lious ave\n",
            "Wer'dor' wod y:\n",
            "\n",
            "Henkns ges wise we me y to elil'doug p in t her spalisusin t wndalu?Y!\n",
            "\n",
            "CKINENGLOFrkeang-lumod n odas ine a! thayayor hannd t; frat.\n",
            "OLArZAUSum,\n",
            "s I f pin hondecharvyouke helldid t we keicetlot lll\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see we are starting to get something reasonableish. It is a dramatic improvement from the gibberish that wa above, but still not full sentences. This is a very very simple model since the tokens are not talking to eachother, it only looks at the last character to make the prediction. So now we want the tokens to talk to eachother which will kick off the transformer!! :) You are doing a great job you have made substantial progress\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "am1gf9ojNym-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Took a detour to understand the mathematical trick in self-attention. I have created a separte google colab notebook for that. It is located in the GitHub repository.\n",
        "\n",
        "Breif Summary:\n",
        "You can do weighted aggregations of your past elements by using matrix multiplication of a lower triangular fashion. The elements in the lower triangular part tells you how much each element uses\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**"
      ],
      "metadata": {
        "id": "ySR7nUbAgYd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self attention\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "X = torch.randn(B,T,C)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ X\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "id": "COxOG1w4n-vU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c44edc3b-ee74-4ea8-a8ef-3ad8c8727340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self attention is explained above, this is notes explaining how the code will work moving foward. As we know attention averages all the past and current tokens to predict the next token. However, each token will have a varying level of attraction to different past tokens. For example, if the current token is a vowel, it may have higher attraction to a consonant compared to another vowel. How do you know which consonant of the past to choose? We want to gather information of the past, but in a data department way. This is where self attention comes in to play\n",
        "\n",
        "Every token emits 2 vectors.\n",
        "\n",
        " 1. A query ~ what am I looking for?\n",
        " 2. A key ~ what do I contain?\n",
        "\n",
        " The way we get affinities (level of attraction) between 2 tokens is by doing a dot product between the keys and the queries. For example, a query from token 20 would dot product with the keys from tokens 0-19.\n",
        "\n",
        " That dot product gets you the level affinity, in the code above we store it as wei. If the key and query allign they will interact to a high amount and you will learn more about that specifc token. For example, if token 20 interacts with token 10 the most, token 20 will learn more about token 10 compared to any other token in that sequence"
      ],
      "metadata": {
        "id": "v_-tdvLysBxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing what we just explained\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x) # size would become (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "# when you foward this Linear ontop of this x, all the tokens in all the positions in the\n",
        "# B by T arrangement, in parallel and independently, produce a key and a query. No communication has happened yet\n",
        "\n",
        "# Now for communication! All the queries will dot product with all the keys\n",
        "wei = q @ k.transpose(-2,-1) # (B, T, 16) @ (B, 16, T) = (B, T, T)... @ means multiply\n",
        "# so for every row of B, we are going to have a T^2 matrix giving us the affinities, which are the wei\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ X\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "id": "La7dFCJgowhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d75628-e483-476e-b1fd-f06981dfb472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei\n",
        "# every batch element has different wei (is not the same anymore) since they contain different tokens at diferent positions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4XfkNTpxPzD",
        "outputId": "47a0c677-bcee-467e-e28e-9a612842d49e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
              "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
              "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
              "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
              "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
              "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
              "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]\n",
        "\n",
        "# for example. In the last row, the 8th token knows what content it has and what position\n",
        "# it is in. So the 8th token creates a query saying what it is looking for. \"I'm a vowel\n",
        "# in the 8th position looking for any consonants in positions up to 4.\" Now all the tokens\n",
        "# emit keys says and maybe one of the channels replies, \"I am consonant and I am in a\n",
        "# position up to 4.\" That key would have a high number in that specifc channel.\n",
        "# So the query and the key when they dot product have a high affinity.\n",
        "\n",
        "# In the last row the 4th token was interesting to the 8th token. So through the softmax\n",
        "# the 8th token will end up aggegating a lot of its information into its position, so\n",
        "# it learns a lot about it."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4a2dSA3xVwh",
        "outputId": "b487bde2-06d2-46bb-c119-102535a0f264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is wei after masking and softmax has already happend, so to look under the hood we will comment out a few lines to see what is happening"
      ],
      "metadata": {
        "id": "FsnQfgOFz5BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "\n",
        "wei = q @ k.transpose(-2,-1)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "#wei = torch.zeros((T,T))\n",
        "#wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "#wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ X\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec0Y9BKIxzkR",
        "outputId": "bd31ed81-f22c-4df3-99a0-9f29c34d1638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]\n",
        "# these are the raw outputs of the dot products. The raw affinities between all the tokens.\n",
        "\n",
        "# we don't want the 5th token to be interacting to the 6,7,8th nodes so we mask those.\n",
        "# look below for results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvwIHEebz1dB",
        "outputId": "ac1078df-6f90-4142-82ca-75afc1cf850b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
              "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
              "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
              "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
              "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
              "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
              "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
              "        [-1.8044, -0.4126, -0.8306,  0.5899, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "\n",
        "wei = q @ k.transpose(-2,-1)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "#wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ X\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PH7ERbnz2E0",
        "outputId": "e98f7f0d-53dd-4e18-a04b-261fa0bf7493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]\n",
        "# Using masking we make sure the past cannot interact with the future\n",
        "# to make sure we have a nice distribution we apply softmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shr-Rzmq2OW1",
        "outputId": "c82fc877-6787-4939-936b-71e857964fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-3.3334, -1.6556,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-1.0226, -1.2606,  0.0762,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [ 0.7836, -0.8014, -0.3368, -0.8496,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,    -inf,    -inf,    -inf],\n",
              "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,    -inf,    -inf],\n",
              "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,    -inf],\n",
              "        [-1.8044, -0.4126, -0.8306,  0.5899, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "\n",
        "wei = q @ k.transpose(-2,-1)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ X\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3xCSANa2O1K",
        "outputId": "9c587fcf-d911-40a6-99a1-9e953c4fce37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]\n",
        "# after exponentiating and normalizing using softmax we get a nice distribution that\n",
        "# sums to 1."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MchN6sIE2nWd",
        "outputId": "c7f2568f-826d-4caf-ee08-27852d88c1bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This final product tells us in a data dependent matter how much information to aggregate from any of the tokens in the past\n",
        "\n",
        "\n",
        "There is one more part to a single self-attention Head. When you do the aggregation you don't actually aggregate the tokens exacly. We produce one more value, we will call in value lol"
      ],
      "metadata": {
        "id": "hpPlDRyx3QLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x) # size would become (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "# when you foward this Linear ontop of this x, all the tokens in all the positions in the\n",
        "# B by T arrangement, in parallel and independently, produce a key and a query. No communication has happened yet\n",
        "\n",
        "# Now for communication! All the queries will dot product with all the keys\n",
        "wei = q @ k.transpose(-2,-1) # (B, T, 16) @ (B, 16, T) = (B, T, T)... @ means multiply\n",
        "# so for every row of B, we are going to have a T^2 matrix giving us the affinities, which are the wei\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "#wei = torch.zeros((T,T)) # not zeroes an more so we comment it out\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x) # don't aggregate x, we calculate a value 'v'\n",
        "out = wei @ v # we output wei multiplied by v\n",
        "# v is the vectors that we aggregate instead of the raw x\n",
        "      # x is like the private information to a token\n",
        "\n",
        "# out = wei @ X\n",
        "\n",
        "out.shape\n",
        "\n",
        "# note that the output of a single Head is 16 not 32 now because it is 16 dimensional (head_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1sXfItO2oNe",
        "outputId": "39f98523-1ebd-4e0e-d226-8c461723bfcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To explain through an example\n",
        "\n",
        "If you are a 5th token with some identity and my information is kept in vector x. For the purposes of this single Head here is what I am interested in, here is what I have, and if you find me interesting here is what I will communicate to you.\n",
        "\n",
        "What you will communicate is stored in v. So v is what gets aggregated for the purpose of this single Head between the different tokens"
      ],
      "metadata": {
        "id": "GCbypted5I0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is the self-attention mechanism, it is probably the hardest part so good job for getting through it!\n",
        "\n",
        "\n",
        "Here are the final notes on attention that Andrej Karpathy wrote in his code:\n",
        "\n",
        "\n",
        "\n",
        "*   Attention is a **communication mechanism**. Can be seen as nodes in a direct graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "*   There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "* Each example across batch dimensions is of course processed completely independently and never \"talk\" to each other. (if there are 4 batches and 32 channels, picture it as 4 separate batches of 8 that talk within themselves but not too eachother)\n",
        "* In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "* \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "* \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ImJuWYV255vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scaled attention\n",
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2,-1)  * head_size**-0.5 # 0.5 is square roo and - makes it 1 over"
      ],
      "metadata": {
        "id": "Z_dkIIRv4w7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PedmWk06_bp0",
        "outputId": "2b50c316-8b2c-489d-a3dd-4719210ea240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiDYBNYq_c0b",
        "outputId": "85848e78-aa34-4136-af36-4f8436f161a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JiCNoJl_dsd",
        "outputId": "61061c8a-29ee-4ee5-ef4e-26361c05e878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n",
        "# input low numbers toward 0 and get low numbers back"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCGUS4Ok_gam",
        "outputId": "2048ab7b-c6ac-41a8-f4df-676e849414aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)\n",
        "# increased the low numbers and it sharpened the max to be more precise\n",
        "# scaling is used to control the variance at initialization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9EmF-hN_qEm",
        "outputId": "a0fb17cb-beb9-4edb-df79-d23740256daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the work after this was switched to Visual Studio Code (code in GitHub file) because it is easier to work with. We added a multiple head attention module, FeedForward module, and Block module.\n",
        "\n",
        " I came back here to go over LayerNorm. It is very similiar to Batch Norm."
      ],
      "metadata": {
        "id": "mtE2Qlfg5gm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm1d:\n",
        "  def __init__(self, dim, eps=1e-5, momentim=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "      return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = BatchNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100 dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "id": "LwLuOiv1AjE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8677e149-18f0-4bbf-84d1-af8ff8cf5609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean, std of one feature acros all batch inputs"
      ],
      "metadata": {
        "id": "SVxhYDyoA7jo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f9afb9-befc-4fa7-bdff-a9c4236b60e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean, std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bOuU9RI7mj0",
        "outputId": "da6d32f7-b891-4123-ceb2-8b58b9f6e107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarize what I have done.\n",
        "\n",
        "I trained a decoder only model following the paper \"Attention is all you need\".\n",
        "I trained the model on the tiny shakespeare data set and got sensable results (would have been better with a GPU). All the training code is about 200 lines of code. Architecturally speaking this code is almost identical to large GPT models like GPT3, with the biggest difference being those large models are anywhwere from ten thousand to 1 million times bigger to what we have here.\n",
        "\n",
        "The next step would be fine tuning. Which could look like getting a GPT in a question/answer format, getting it to preform tasks, detecting sentiment, etc. That is the harder step which could be supervised fine tuning or something much more complex like creating a reward model to train how Open AI does."
      ],
      "metadata": {
        "id": "tehGOil-aLbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So I finsihed the video!!! But I am calling it quits so next time what we have to do is this.\n",
        "\n",
        "1.   Upload what you have done to github\n",
        "2.   redo it without the comments in a separate notebook in colab and VS code\n",
        "3. read the article and right about it on linkedIn\n",
        "4. make a post about project on linkedIn about\n",
        "5. Then you are done!\n",
        "\n",
        "\n",
        "Get started on mirror after that and see if that is possible! You are killing it first summer project is almost done :)"
      ],
      "metadata": {
        "id": "-xY43Vvacypx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AunFxAejcwSr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
